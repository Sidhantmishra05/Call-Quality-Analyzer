{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "HC_poq9f8LtG"
      },
      "outputs": [],
      "source": [
        "!pip install pytube moviepy openai-whisper pyannote.audio torch transformers librosa --quiet\n",
        "\n",
        "import os, torch\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "from moviepy.editor import AudioFileClip\n",
        "from transformers import pipeline\n",
        "from pyannote.audio import Pipeline\n",
        "from collections import defaultdict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download & Preprocess Audio\n",
        "!pip install yt-dlp --quiet\n",
        "\n",
        "import yt_dlp\n",
        "\n",
        "def download_youtube_audio(url, out_file=\"call.wav\"):\n",
        "    \"\"\"\n",
        "    Downloads audio from a YouTube video and converts it to WAV format.\n",
        "    Uses yt-dlp for reliability.\n",
        "    \"\"\"\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'outtmpl': 'call_audio.%(ext)s',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "\n",
        "    # Rename to consistent output\n",
        "    if os.path.exists(\"call_audio.wav\"):\n",
        "        os.rename(\"call_audio.wav\", out_file)\n",
        "\n",
        "    return out_file\n",
        "\n",
        "# Example usage\n",
        "yt_url = \"https://www.youtube.com/watch?v=4ostqJD3Psc\"\n",
        "audio_file = download_youtube_audio(yt_url)\n",
        "print(\"âœ… Audio saved as:\", audio_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rhsubif8h2d",
        "outputId": "8c9e3177-6853-4235-e5a4-b9416a6ea2f8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=4ostqJD3Psc\n",
            "[youtube] 4ostqJD3Psc: Downloading webpage\n",
            "[youtube] 4ostqJD3Psc: Downloading tv simply player API JSON\n",
            "[youtube] 4ostqJD3Psc: Downloading tv client config\n",
            "[youtube] 4ostqJD3Psc: Downloading tv player API JSON\n",
            "[info] 4ostqJD3Psc: Downloading 1 format(s): 251\n",
            "[download] Destination: call_audio.webm\n",
            "[download] 100% of    1.99MiB in 00:00:00 at 11.46MiB/s  \n",
            "[ExtractAudio] Destination: call_audio.wav\n",
            "Deleting original file call_audio.webm (pass -k to keep)\n",
            "âœ… Audio saved as: call.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GROUP 3: Speech-to-Text (Whisper)\n",
        "# --------------------------\n",
        "def transcribe_audio(audio_path, model_size=\"small\"):\n",
        "    model = whisper.load_model(model_size)\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result[\"text\"], result[\"segments\"]\n",
        "\n",
        "transcript, segments = transcribe_audio(audio_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IpRDFQh94RX",
        "outputId": "f0bf99d3-a5a5-4576-c855-f45eccdd9522"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cae441ab"
      },
      "source": [
        "### Combine Transcription and Diarization\n",
        "\n",
        "Now that we have the transcription segments and the speaker diarization segments, we can combine them to create a final transcript with speaker labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1c59ad2",
        "outputId": "a8683c83-b731-4d78-83f0-14d82c030850"
      },
      "source": [
        "def combine_transcription_diarization(segments, speaker_segments):\n",
        "    \"\"\"\n",
        "    Combines transcription segments with speaker diarization segments.\n",
        "\n",
        "    Parameters:\n",
        "        segments (list): List of transcription segments from Whisper.\n",
        "        speaker_segments (list): List of speaker segments from diarization.\n",
        "\n",
        "    Returns:\n",
        "        list: List of combined segments with speaker labels and text.\n",
        "    \"\"\"\n",
        "    combined_segments = []\n",
        "    speaker_index = 0\n",
        "\n",
        "    for segment in segments:\n",
        "        segment_start = segment['start']\n",
        "        segment_end = segment['end']\n",
        "        segment_text = segment['text'].strip()\n",
        "\n",
        "        # Find the speaker for this transcription segment\n",
        "        current_speaker = \"Unknown Speaker\"\n",
        "        for i in range(speaker_index, len(speaker_segments)):\n",
        "            diarization_start = speaker_segments[i]['start']\n",
        "            diarization_end = speaker_segments[i]['end']\n",
        "\n",
        "            # Check for overlap between transcription segment and diarization segment\n",
        "            if max(segment_start, diarization_start) < min(segment_end, diarization_end):\n",
        "                current_speaker = speaker_segments[i]['speaker']\n",
        "                speaker_index = i  # Start searching from this speaker segment next time\n",
        "                break\n",
        "\n",
        "        combined_segments.append({\n",
        "            \"speaker\": current_speaker,\n",
        "            \"start\": segment_start,\n",
        "            \"end\": segment_end,\n",
        "            \"text\": segment_text\n",
        "        })\n",
        "\n",
        "    return combined_segments\n",
        "\n",
        "# Assuming 'segments' is the output from the transcribe_audio function\n",
        "# and 'speaker_segments' is the output from the diarize_speakers function\n",
        "# (using the corrected code from the previous turn if needed)\n",
        "\n",
        "final_transcript = combine_transcription_diarization(segments, speaker_segments)\n",
        "\n",
        "# Print the final transcript\n",
        "for entry in final_transcript:\n",
        "    print(f\"[{entry['start']:.2f}s - {entry['end']:.2f}s] {entry['speaker']}: {entry['text']}\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00s - 9.36s] Speaker_1: Thank you for calling Nissan.\n",
            "[9.36s - 10.36s] Speaker_2: My name is Lauren.\n",
            "[10.36s - 11.36s] Speaker_2: Can I have your name?\n",
            "[11.36s - 14.16s] Speaker_2: Yeah, my name is John Smith.\n",
            "[14.16s - 15.16s] Speaker_2: Thank you, John.\n",
            "[15.16s - 16.16s] Speaker_1: How can I help you?\n",
            "[16.16s - 20.60s] Speaker_2: I was just calling about to see how much it would cost to update the map in my car.\n",
            "[20.60s - 22.48s] Speaker_2: I'd be happy to help you with that today.\n",
            "[22.48s - 24.00s] Speaker_1: Did you receive a mail or from us?\n",
            "[24.00s - 25.00s] Speaker_2: I did.\n",
            "[25.00s - 26.48s] Speaker_1: Do you need the customer number?\n",
            "[26.48s - 27.48s] Speaker_1: Yes, please.\n",
            "[27.48s - 28.48s] Speaker_1: Okay.\n",
            "[28.48s - 31.00s] Speaker_1: I have a 15243.\n",
            "[31.00s - 32.00s] Speaker_1: Thank you.\n",
            "[32.00s - 33.32s] Speaker_1: And the year-making model of your vehicle?\n",
            "[33.32s - 36.88s] Speaker_1: Yeah, I have a 2009 Nissan Altima.\n",
            "[36.88s - 38.36s] Speaker_2: Oh, nice car.\n",
            "[38.36s - 39.52s] Speaker_2: Yeah, thank you.\n",
            "[39.52s - 40.52s] Speaker_2: We really enjoy it.\n",
            "[40.52s - 43.04s] Speaker_2: Okay, I think I found your profile here.\n",
            "[43.04s - 46.48s] Speaker_2: Can I have you verify your address and phone number, please?\n",
            "[46.48s - 47.48s] Speaker_2: Yes.\n",
            "[47.48s - 50.12s] Speaker_1: It's 1255 North Research Way.\n",
            "[50.12s - 52.80s] Speaker_2: That's in Orem, Utah, 84097.\n",
            "[53.80s - 57.68s] Speaker_1: And my phone number is A01-431-1000.\n",
            "[57.68s - 58.68s] Speaker_2: Thanks, John.\n",
            "[58.68s - 61.18s] Speaker_1: I located your information.\n",
            "[61.18s - 66.04s] Speaker_2: The newest version we have available for your vehicle is version 7.7, which was released\n",
            "[66.04s - 68.56s] Speaker_2: in March of 2012.\n",
            "[68.56s - 72.40s] Speaker_2: The price of the new map is $99 plus shipping and tax.\n",
            "[72.40s - 74.80s] Speaker_2: Let me go ahead and set up this order for you.\n",
            "[74.80s - 76.60s] Speaker_1: Well, can we wait just a second?\n",
            "[76.60s - 79.04s] Speaker_2: I'm not really sure if I can afford it right now.\n",
            "[79.04s - 82.92s] Speaker_1: All right, well, here are a few reasons to consider purchasing today.\n",
            "[82.92s - 86.76s] Speaker_2: It looks as though you haven't updated your vehicle for three years.\n",
            "[86.76s - 90.52s] Speaker_2: So that would be the equivalent of getting three years worth of updates for the price\n",
            "[90.52s - 91.52s] Speaker_1: of one.\n",
            "[91.52s - 92.52s] Speaker_1: Oh, okay.\n",
            "[92.52s - 97.00s] Speaker_2: In addition, special offers like the current promotion don't come around too often.\n",
            "[97.00s - 101.80s] Speaker_2: I would definitely recommend taking advantage of the extra $50 off before it expires.\n",
            "[101.80s - 104.16s] Speaker_1: Yeah, that does sound pretty good.\n",
            "[104.28s - 109.60s] Speaker_2: If I set this order up for you now, it'll ship out today and for $50 less.\n",
            "[109.60s - 113.44s] Speaker_2: Do you have your credit card handy and I can place this order for you now?\n",
            "[113.44s - 117.64s] Speaker_1: Yeah, let's go ahead and use a visa.\n",
            "[117.64s - 118.64s] Speaker_1: My number is...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GROUP 5: Metrics Extraction\n",
        "# --------------------------\n",
        "def compute_metrics(transcript, speaker_segments):\n",
        "    # Talk-time ratio\n",
        "    talk_time = defaultdict(float)\n",
        "    for seg in speaker_segments:\n",
        "        talk_time[seg[\"speaker\"]] += seg[\"duration\"]\n",
        "    total_time = sum(talk_time.values()) or 1\n",
        "    talk_ratio = {sp: round((dur/total_time)*100, 2) for sp, dur in talk_time.items()}\n",
        "\n",
        "    # Longest monologue\n",
        "    longest_mono = max(speaker_segments, key=lambda x: x[\"duration\"])\n",
        "\n",
        "    # Questions count\n",
        "    question_count = transcript.count(\"?\")\n",
        "\n",
        "    return talk_ratio, longest_mono, question_count\n",
        "\n",
        "talk_ratio, longest_mono, question_count = compute_metrics(transcript, speaker_segments)\n"
      ],
      "metadata": {
        "id": "nuGax8WvHY__"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # GROUP 6: Sentiment Analysis\n",
        "# --------------------------\n",
        "def analyze_sentiment(text, sample_size=500):\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "    sample_text = text[:sample_size]\n",
        "    return sentiment_analyzer(sample_text)[0][\"label\"]\n",
        "\n",
        "sentiment = analyze_sentiment(transcript)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3ZPUtR7IdlH",
        "outputId": "b3e59e7d-3b78-42c7-8d34-8889905644fe"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GROUP 7: Actionable Insight + Bonus\n",
        "# --------------------------\n",
        "def generate_insight(talk_ratio, question_count):\n",
        "    if not talk_ratio:\n",
        "        return \"Insufficient data.\"\n",
        "\n",
        "    dominant = max(talk_ratio, key=talk_ratio.get)\n",
        "    if talk_ratio[dominant] > 70:\n",
        "        return f\"{dominant} dominated the call. Encourage balanced dialogue.\"\n",
        "    elif question_count < 3:\n",
        "        return \"Too few questions. Encourage rep to ask more.\"\n",
        "    else:\n",
        "        return \"Good balance of talk and engagement.\"\n",
        "\n",
        "insight = generate_insight(talk_ratio, question_count)\n",
        "\n",
        "# Bonus: Identify Sales Rep vs Customer\n",
        "sales_rep = max(talk_ratio, key=talk_ratio.get, default=\"Unknown\")\n",
        "customer = min(talk_ratio, key=talk_ratio.get, default=\"Unknown\")"
      ],
      "metadata": {
        "id": "L3FrPEH3IknV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GROUP 8: Results\n",
        "# --------------------------\n",
        "print(\"ðŸ“Š Talk-time Ratio:\", talk_ratio)\n",
        "print(\"â“ Questions Asked:\", question_count)\n",
        "print(\"ðŸ—£ï¸ Longest Monologue:\", round(longest_mono['duration'], 2), \"seconds by\", longest_mono['speaker'])\n",
        "print(\"ðŸ˜Š Call Sentiment:\", sentiment)\n",
        "print(\"ðŸ’¡ Actionable Insight:\", insight)\n",
        "print(\"ðŸ‘¤ Likely Sales Rep:\", sales_rep, \"| Customer:\", customer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8CnRgXbIqiv",
        "outputId": "872d00d4-6f90-49bd-eb8e-17d46d2a0b5a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š Talk-time Ratio: {'Speaker_1': np.float64(51.58), 'Speaker_2': np.float64(48.42)}\n",
            "â“ Questions Asked: 8\n",
            "ðŸ—£ï¸ Longest Monologue: 1.71 seconds by Speaker_2\n",
            "ðŸ˜Š Call Sentiment: POSITIVE\n",
            "ðŸ’¡ Actionable Insight: Good balance of talk and engagement.\n",
            "ðŸ‘¤ Likely Sales Rep: Speaker_1 | Customer: Speaker_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2549e879"
      },
      "source": [
        "### Finish Task\n",
        "\n",
        "The task is complete. We have successfully downloaded the audio from the YouTube video, transcribed it, performed speaker diarization, and combined the results to produce a transcript with speaker labels."
      ]
    }
  ]
}